# -*- coding: utf-8 -*-
"""domain_qa_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KdgYtfcTmYKJlawAsFN76fSZZGUeD44G

# LLM-Powered Domain-Specific QA Agent
"""

# ü©∫ DOMAIN Q&A CHATBOT USING MEDQUAD DATASET (HEALTH DOMAIN)


# STEP 1: Install required libraries
!pip install -q sentence-transformers faiss-cpu transformers gradio datasets accelerate --upgrade

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from transformers import pipeline
import gradio as gr

# STEP 2: Load the MedQuAD Dataset

try:
    df = pd.read_csv("medquad.csv")
except FileNotFoundError:
    print("Error: 'medquad.csv' not found. Please upload the dataset file.")
    # Exit or handle the error appropriately if the file is not found
    exit()

# Standardize column names
df = df.rename(columns={"Question": "question", "Answer": "answer"})
df["source"] = "NIH MedQuAD"

if "question" not in df.columns or "answer" not in df.columns:
    print("Error: 'medquad.csv' must contain 'Question' and 'Answer' columns.")
    exit()

df = df[["question", "answer", "source"]]

# Show first few rows
print("‚úÖ Dataset loaded successfully!")
print(df.head())

# STEP 3: Build Sentence Embeddings and FAISS Index

print("\nüîÑ Creating sentence embeddings...")

# Use MiniLM model for lightweight but accurate embeddings
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Encode all questions into embeddings
# Handle potential non-string data in 'question' column
questions = df["question"].astype(str).tolist()
question_embeddings = embedder.encode(questions, show_progress_bar=True)

# Build FAISS index for similarity search
index = faiss.IndexFlatL2(question_embeddings.shape[1])
index.add(np.array(question_embeddings))

print(f"‚úÖ FAISS index built with {index.ntotal} medical Q&A entries.")

# STEP 4: Define Semantic Search Function

def semantic_search(query, top_k=3):
    """Find top_k semantically similar questions"""
    query_embedding = embedder.encode([query])
    distances, indices = index.search(np.array(query_embedding), top_k)
    results = []
    for i, idx in enumerate(indices[0]):
        results.append({
            "question": df.iloc[idx]["question"],
            "answer": df.iloc[idx]["answer"],
            "source": df.iloc[idx]["source"],
            "distance": float(distances[0][i])
        })
    return results

# STEP 5: Add QA Model for Contextual Answer Refinement

print("\nü§ñ Loading QA model...")
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

def answer_query(query):
    """Retrieve best context and refine answer"""
    results = semantic_search(query, top_k=1)
    if not results:
        return {"original_answer": "No relevant information found.", "refined_answer": "No relevant information found.", "source": ""}

    best = results[0]
    context = str(best["answer"]) # Ensure context is a string
    # Check if context is not empty or just whitespace before passing to pipeline
    if not context.strip():
         return {"original_answer": best["answer"], "refined_answer": "Could not refine answer based on provided context.", "source": best["source"]}


    try:
        refined = qa_pipeline(question=query, context=context)
        refined_answer = refined["answer"]
    except Exception as e:
        print(f"Error during QA pipeline processing: {e}")
        refined_answer = "Could not refine answer." # Provide a fallback message

    return {
        "original_answer": best["answer"],
        "refined_answer": refined_answer,
        "source": best["source"]
    }

# Test example
print("\nüß† Example test:")
test_query = "What are the early signs of diabetes?"
test_result = answer_query(test_query)
print(f"Query: {test_query}")
print(f"Original Answer: {test_result['original_answer']}")
print(f"Refined Answer: {test_result['refined_answer']}")
print(f"Source: {test_result['source']}")

# STEP 6: Launch Gradio Chatbot Interface

def chat_interface(query):
    if not query.strip():
        return "‚ùó Please enter a question."
    result = answer_query(query)
    return f"**Answer:** {result['refined_answer']}\n\n**Source:** {result['source']}"

# Create Gradio Interface
gr.Interface(
    fn=chat_interface,
    inputs="text",
    outputs="markdown",
    title="ü©∫ MedQuAD Medical Q&A Chatbot",
    description="Ask any health-related question ‚Äî the bot finds and refines answers using NIH MedQuAD dataset.",
    theme="default"
).launch(share=True)